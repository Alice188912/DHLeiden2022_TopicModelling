{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "Topic modelling is an unsupervised machine learning technique used to discover topics that are present in a corpus. This is known as 'unsupervised' machine learning because it doesnâ€™t require training data that has previously been annotated and classified by humans. Topic modelling involves counting words and grouping similar word patterns to infer topics within unstructured data. \n",
    "\n",
    "We will learn how to do topic modelling in Python using <b>Latent Dirichlet Allocation (LDA)</b> and <b>Non-negative Matrix Factorization (NMF)</b> implemented in the scikit library. While LDA and NMF are based on different mathematical concepts, both algorithms are able to return the documents that belong to a topic in a corpus and the words that belong to a topic. \n",
    "\n",
    "Run all code cells in the given order. \n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "First we import all Python libraries/modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # prepares tokens for use in topic model\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # prepares tokens for use in topic model\n",
    "from sklearn.decomposition import NMF \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will open the folder 'BBC' that contains a collection of news texts and get a list of all the files (i.e. texts) that are stored in that folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "['BBC\\\\001.txt', 'BBC\\\\002.txt', 'BBC\\\\003.txt', 'BBC\\\\004.txt', 'BBC\\\\005.txt', 'BBC\\\\006.txt', 'BBC\\\\007.txt', 'BBC\\\\008.txt', 'BBC\\\\009.txt', 'BBC\\\\010.txt']\n"
     ]
    }
   ],
   "source": [
    "file_names=sorted([os.path.join(\"BBC\", fn) for fn in os.listdir(\"BBC\") if fn.endswith(\".txt\")])\n",
    "print(len(file_names)) # count files in corpus\n",
    "print(file_names[:10]) # print names of 1st ten files in corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "In this section we will see how to do topic modelling using LDA, also implemented in the scikit Python library. To prepare texts for the LDA topic modelling, we use a <i>CountVectorizer</i> vectorizer (LDA works with raw frequencies). With the help of <i>CountVectorizer</i> we will tokenize the text, put all characters lower case, remove stop words and count the frequency of each token (word).  <i>CountVectorizer</i> will turn a collection of text documents into numerical feature vectors, where a corpus of documents is represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 4221\n",
      "Feature examples: ['gough', 'governance', 'governing', 'government', 'governor', 'graham', 'grand', 'granted', 'gray', 'great']\n"
     ]
    }
   ],
   "source": [
    "# LDA can only use raw term counts. It is a probabilistic graphical model.\n",
    "lda_vectorizer = CountVectorizer(input='filename', analyzer='word', max_df=0.95, min_df=2, stop_words='english')\n",
    "lda_features = lda_vectorizer.fit_transform(file_names)\n",
    "lda_feature_names = lda_vectorizer.get_feature_names()\n",
    "\n",
    "# show total number of features\n",
    "print(\"Number of features:\", len(lda_feature_names))\n",
    "\n",
    "# show some examples\n",
    "print(\"Feature examples:\", lda_feature_names[1700:1710])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the LDA topic model and apply it to the features (words). We also print the first 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          max_iter=5, random_state=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "lda_model.fit(lda_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that shows the first n words in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will print the first 10 words in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "sales said year profits 2004 euros india rose spending growth\n",
      "\n",
      "Topic #1:\n",
      "company mr euros charges marsh bankruptcy parmalat business group firm\n",
      "\n",
      "Topic #2:\n",
      "fuel said fiat airlines drugs engines tax year deutsche gm\n",
      "\n",
      "Topic #3:\n",
      "seed rusedski number forced second rib said left half victory\n",
      "\n",
      "Topic #4:\n",
      "said year growth new economy oil market government years economic\n",
      "\n",
      "Topic #5:\n",
      "said win mirza set final roddick world game open year\n",
      "\n",
      "Topic #6:\n",
      "car mini factory cars 000 gm saab bmw new production\n",
      "\n",
      "Topic #7:\n",
      "mci airlines verizon fiat qwest passengers new offer compensation beer\n",
      "\n",
      "Topic #8:\n",
      "fiat gm prices oil said crude opec firm cut barrel\n",
      "\n",
      "Topic #9:\n",
      "dollar ireland south gara reserves said korea horgan penalty minute\n"
     ]
    }
   ],
   "source": [
    "# print top 10 topics\n",
    "print_topics(lda_model, lda_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also check the highest ranked topic for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1: topic: 0\n",
      "\n",
      "doc 2: topic: 4\n",
      "\n",
      "doc 3: topic: 4\n",
      "\n",
      "doc 4: topic: 0\n",
      "\n",
      "doc 5: topic: 4\n",
      "\n",
      "doc 6: topic: 4\n",
      "\n",
      "doc 7: topic: 4\n",
      "\n",
      "doc 8: topic: 4\n",
      "\n",
      "doc 9: topic: 4\n",
      "\n",
      "doc 10: topic: 4\n",
      "\n",
      "doc 11: topic: 0\n",
      "\n",
      "doc 12: topic: 4\n",
      "\n",
      "doc 13: topic: 4\n",
      "\n",
      "doc 14: topic: 4\n",
      "\n",
      "doc 15: topic: 7\n",
      "\n",
      "doc 16: topic: 4\n",
      "\n",
      "doc 17: topic: 1\n",
      "\n",
      "doc 18: topic: 0\n",
      "\n",
      "doc 19: topic: 3\n",
      "\n",
      "doc 20: topic: 3\n",
      "\n",
      "doc 21: topic: 4\n",
      "\n",
      "doc 22: topic: 4\n",
      "\n",
      "doc 23: topic: 4\n",
      "\n",
      "doc 24: topic: 4\n",
      "\n",
      "doc 25: topic: 4\n",
      "\n",
      "doc 26: topic: 4\n",
      "\n",
      "doc 27: topic: 4\n",
      "\n",
      "doc 28: topic: 4\n",
      "\n",
      "doc 29: topic: 4\n",
      "\n",
      "doc 30: topic: 4\n",
      "\n",
      "doc 31: topic: 0\n",
      "\n",
      "doc 32: topic: 4\n",
      "\n",
      "doc 33: topic: 1\n",
      "\n",
      "doc 34: topic: 4\n",
      "\n",
      "doc 35: topic: 4\n",
      "\n",
      "doc 36: topic: 5\n",
      "\n",
      "doc 37: topic: 1\n",
      "\n",
      "doc 38: topic: 0\n",
      "\n",
      "doc 39: topic: 0\n",
      "\n",
      "doc 40: topic: 0\n",
      "\n",
      "doc 41: topic: 4\n",
      "\n",
      "doc 42: topic: 2\n",
      "\n",
      "doc 43: topic: 4\n",
      "\n",
      "doc 44: topic: 4\n",
      "\n",
      "doc 45: topic: 4\n",
      "\n",
      "doc 46: topic: 7\n",
      "\n",
      "doc 47: topic: 0\n",
      "\n",
      "doc 48: topic: 4\n",
      "\n",
      "doc 49: topic: 4\n",
      "\n",
      "doc 50: topic: 2\n",
      "\n",
      "doc 51: topic: 4\n",
      "\n",
      "doc 52: topic: 4\n",
      "\n",
      "doc 53: topic: 4\n",
      "\n",
      "doc 54: topic: 4\n",
      "\n",
      "doc 55: topic: 4\n",
      "\n",
      "doc 56: topic: 0\n",
      "\n",
      "doc 57: topic: 1\n",
      "\n",
      "doc 58: topic: 4\n",
      "\n",
      "doc 59: topic: 4\n",
      "\n",
      "doc 60: topic: 4\n",
      "\n",
      "doc 61: topic: 4\n",
      "\n",
      "doc 62: topic: 4\n",
      "\n",
      "doc 63: topic: 4\n",
      "\n",
      "doc 64: topic: 4\n",
      "\n",
      "doc 65: topic: 4\n",
      "\n",
      "doc 66: topic: 4\n",
      "\n",
      "doc 67: topic: 4\n",
      "\n",
      "doc 68: topic: 4\n",
      "\n",
      "doc 69: topic: 5\n",
      "\n",
      "doc 70: topic: 4\n",
      "\n",
      "doc 71: topic: 4\n",
      "\n",
      "doc 72: topic: 0\n",
      "\n",
      "doc 73: topic: 0\n",
      "\n",
      "doc 74: topic: 4\n",
      "\n",
      "doc 75: topic: 4\n",
      "\n",
      "doc 76: topic: 4\n",
      "\n",
      "doc 77: topic: 0\n",
      "\n",
      "doc 78: topic: 4\n",
      "\n",
      "doc 79: topic: 9\n",
      "\n",
      "doc 80: topic: 0\n",
      "\n",
      "doc 81: topic: 4\n",
      "\n",
      "doc 82: topic: 4\n",
      "\n",
      "doc 83: topic: 2\n",
      "\n",
      "doc 84: topic: 4\n",
      "\n",
      "doc 85: topic: 0\n",
      "\n",
      "doc 86: topic: 4\n",
      "\n",
      "doc 87: topic: 4\n",
      "\n",
      "doc 88: topic: 0\n",
      "\n",
      "doc 89: topic: 4\n",
      "\n",
      "doc 90: topic: 1\n",
      "\n",
      "doc 91: topic: 4\n",
      "\n",
      "doc 92: topic: 0\n",
      "\n",
      "doc 93: topic: 8\n",
      "\n",
      "doc 94: topic: 4\n",
      "\n",
      "doc 95: topic: 7\n",
      "\n",
      "doc 96: topic: 4\n",
      "\n",
      "doc 97: topic: 4\n",
      "\n",
      "doc 98: topic: 4\n",
      "\n",
      "doc 99: topic: 4\n",
      "\n",
      "doc 100: topic: 4\n",
      "\n",
      "doc 101: topic: 4\n",
      "\n",
      "doc 102: topic: 0\n",
      "\n",
      "doc 103: topic: 4\n",
      "\n",
      "doc 104: topic: 4\n",
      "\n",
      "doc 105: topic: 4\n",
      "\n",
      "doc 106: topic: 0\n",
      "\n",
      "doc 107: topic: 0\n",
      "\n",
      "doc 108: topic: 0\n",
      "\n",
      "doc 109: topic: 2\n",
      "\n",
      "doc 110: topic: 4\n",
      "\n",
      "doc 111: topic: 4\n",
      "\n",
      "doc 112: topic: 4\n",
      "\n",
      "doc 113: topic: 9\n",
      "\n",
      "doc 114: topic: 4\n",
      "\n",
      "doc 115: topic: 4\n",
      "\n",
      "doc 116: topic: 4\n",
      "\n",
      "doc 117: topic: 0\n",
      "\n",
      "doc 118: topic: 7\n",
      "\n",
      "doc 119: topic: 4\n",
      "\n",
      "doc 120: topic: 4\n",
      "\n",
      "doc 121: topic: 4\n",
      "\n",
      "doc 122: topic: 4\n",
      "\n",
      "doc 123: topic: 4\n",
      "\n",
      "doc 124: topic: 5\n",
      "\n",
      "doc 125: topic: 4\n",
      "\n",
      "doc 126: topic: 4\n",
      "\n",
      "doc 127: topic: 4\n",
      "\n",
      "doc 128: topic: 4\n",
      "\n",
      "doc 129: topic: 4\n",
      "\n",
      "doc 130: topic: 3\n",
      "\n",
      "doc 131: topic: 3\n",
      "\n",
      "doc 132: topic: 5\n",
      "\n",
      "doc 133: topic: 3\n",
      "\n",
      "doc 134: topic: 5\n",
      "\n",
      "doc 135: topic: 5\n",
      "\n",
      "doc 136: topic: 5\n",
      "\n",
      "doc 137: topic: 5\n",
      "\n",
      "doc 138: topic: 5\n",
      "\n",
      "doc 139: topic: 5\n",
      "\n",
      "doc 140: topic: 5\n",
      "\n",
      "doc 141: topic: 5\n",
      "\n",
      "doc 142: topic: 4\n",
      "\n",
      "doc 143: topic: 5\n",
      "\n",
      "doc 144: topic: 5\n",
      "\n",
      "doc 145: topic: 5\n",
      "\n",
      "doc 146: topic: 5\n",
      "\n",
      "doc 147: topic: 5\n",
      "\n",
      "doc 148: topic: 5\n",
      "\n",
      "doc 149: topic: 5\n",
      "\n",
      "doc 150: topic: 3\n",
      "\n",
      "doc 151: topic: 5\n",
      "\n",
      "doc 152: topic: 5\n",
      "\n",
      "doc 153: topic: 5\n",
      "\n",
      "doc 154: topic: 5\n",
      "\n",
      "doc 155: topic: 5\n",
      "\n",
      "doc 156: topic: 4\n",
      "\n",
      "doc 157: topic: 5\n",
      "\n",
      "doc 158: topic: 4\n",
      "\n",
      "doc 159: topic: 5\n",
      "\n",
      "doc 160: topic: 5\n",
      "\n",
      "doc 161: topic: 5\n",
      "\n",
      "doc 162: topic: 5\n",
      "\n",
      "doc 163: topic: 5\n",
      "\n",
      "doc 164: topic: 5\n",
      "\n",
      "doc 165: topic: 5\n",
      "\n",
      "doc 166: topic: 5\n",
      "\n",
      "doc 167: topic: 5\n",
      "\n",
      "doc 168: topic: 5\n",
      "\n",
      "doc 169: topic: 5\n",
      "\n",
      "doc 170: topic: 5\n",
      "\n",
      "doc 171: topic: 5\n",
      "\n",
      "doc 172: topic: 5\n",
      "\n",
      "doc 173: topic: 5\n",
      "\n",
      "doc 174: topic: 9\n",
      "\n",
      "doc 175: topic: 3\n",
      "\n",
      "doc 176: topic: 5\n",
      "\n",
      "doc 177: topic: 5\n",
      "\n",
      "doc 178: topic: 4\n",
      "\n",
      "doc 179: topic: 3\n",
      "\n",
      "doc 180: topic: 5\n",
      "\n",
      "doc 181: topic: 5\n",
      "\n",
      "doc 182: topic: 5\n",
      "\n",
      "doc 183: topic: 5\n",
      "\n",
      "doc 184: topic: 5\n",
      "\n",
      "doc 185: topic: 5\n",
      "\n",
      "doc 186: topic: 5\n",
      "\n",
      "doc 187: topic: 5\n",
      "\n",
      "doc 188: topic: 5\n",
      "\n",
      "doc 189: topic: 5\n",
      "\n",
      "doc 190: topic: 5\n",
      "\n",
      "doc 191: topic: 5\n",
      "\n",
      "doc 192: topic: 3\n",
      "\n",
      "doc 193: topic: 5\n",
      "\n",
      "doc 194: topic: 5\n",
      "\n",
      "doc 195: topic: 5\n",
      "\n",
      "doc 196: topic: 3\n",
      "\n",
      "doc 197: topic: 5\n",
      "\n",
      "doc 198: topic: 3\n",
      "\n",
      "doc 199: topic: 5\n",
      "\n",
      "doc 200: topic: 5\n",
      "\n",
      "doc 201: topic: 3\n",
      "\n",
      "doc 202: topic: 5\n",
      "\n",
      "doc 203: topic: 5\n",
      "\n",
      "doc 204: topic: 5\n",
      "\n",
      "doc 205: topic: 3\n",
      "\n",
      "doc 206: topic: 5\n",
      "\n",
      "doc 207: topic: 3\n",
      "\n",
      "doc 208: topic: 9\n",
      "\n",
      "doc 209: topic: 9\n",
      "\n",
      "doc 210: topic: 5\n",
      "\n",
      "doc 211: topic: 5\n",
      "\n",
      "doc 212: topic: 5\n",
      "\n",
      "doc 213: topic: 5\n",
      "\n",
      "doc 214: topic: 5\n",
      "\n",
      "doc 215: topic: 5\n",
      "\n",
      "doc 216: topic: 5\n",
      "\n",
      "doc 217: topic: 5\n",
      "\n",
      "doc 218: topic: 5\n",
      "\n",
      "doc 219: topic: 5\n",
      "\n",
      "doc 220: topic: 5\n",
      "\n",
      "doc 221: topic: 5\n",
      "\n",
      "doc 222: topic: 5\n",
      "\n",
      "doc 223: topic: 5\n",
      "\n",
      "doc 224: topic: 5\n",
      "\n",
      "doc 225: topic: 5\n",
      "\n",
      "doc 226: topic: 5\n",
      "\n",
      "doc 227: topic: 5\n",
      "\n",
      "doc 228: topic: 4\n",
      "\n",
      "doc 229: topic: 5\n",
      "\n",
      "doc 230: topic: 5\n",
      "\n",
      "doc 231: topic: 5\n",
      "\n",
      "doc 232: topic: 5\n",
      "\n",
      "doc 233: topic: 5\n",
      "\n",
      "doc 234: topic: 5\n",
      "\n",
      "doc 235: topic: 5\n",
      "\n",
      "doc 236: topic: 5\n",
      "\n",
      "doc 237: topic: 5\n",
      "\n",
      "doc 238: topic: 5\n",
      "\n",
      "doc 239: topic: 5\n",
      "\n",
      "doc 240: topic: 5\n",
      "\n",
      "doc 241: topic: 5\n",
      "\n",
      "doc 242: topic: 5\n",
      "\n",
      "doc 243: topic: 5\n",
      "\n",
      "doc 244: topic: 5\n",
      "\n",
      "doc 245: topic: 5\n",
      "\n",
      "doc 246: topic: 5\n",
      "\n",
      "doc 247: topic: 5\n",
      "\n",
      "doc 248: topic: 5\n",
      "\n",
      "doc 249: topic: 3\n",
      "\n",
      "doc 250: topic: 5\n",
      "\n",
      "doc 251: topic: 5\n",
      "\n",
      "doc 252: topic: 5\n",
      "\n",
      "doc 253: topic: 5\n",
      "\n",
      "doc 254: topic: 5\n",
      "\n",
      "doc 255: topic: 5\n",
      "\n",
      "doc 256: topic: 5\n",
      "\n",
      "doc 257: topic: 3\n",
      "\n",
      "doc 258: topic: 5\n",
      "\n",
      "doc 259: topic: 5\n",
      "\n",
      "doc 260: topic: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_doc_topic = lda_model.transform(lda_features)\n",
    "\n",
    "for n in range(lda_doc_topic.shape[0]):\n",
    "    topic_most_pr_lda = lda_doc_topic[n].argmax()\n",
    "    print(\"doc {}: topic: {}\\n\".format(n+1,topic_most_pr_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Non-negative Matrix Factorization\n",
    "\n",
    "\n",
    "Now we will see how to do topic modelling using Non-negative Matrix Factorization as implemented in the scikit Python library. First we need to put our texts in a bag of words matrix format where each text is represented as a row, and each column contains the count of words in the texts. To prepare such a matrix for the NMF algorithm, we use a <i>TfidfVectorizer</i>. <i>TfidfVectorizer</i> tokenizes texts, counts frequency of each word, calculates <i>tf-idf</i> (Term Frequency Inverse Document Frequency) value for each word in the corpus and puts it in the format needed as an input for the NMF. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 8592\n",
      "Feature examples: ['claude', 'clauses', 'clawed', 'clay', 'clean', 'clear', 'clearance', 'cleared', 'clearer', 'clearest']\n"
     ]
    }
   ],
   "source": [
    "# define the vectorizer\n",
    "nmf_vectorizer = TfidfVectorizer(input='filename', analyzer='word', min_df=1, strip_accents = None, stop_words='english', preprocessor=None, encoding = 'utf-8')\n",
    "\n",
    "# obtain all features from the texts\n",
    "features = nmf_vectorizer.fit_transform(file_names)\n",
    "feature_names = nmf_vectorizer.get_feature_names()\n",
    "\n",
    "# show total number of features\n",
    "print(\"Number of features:\", len(feature_names))\n",
    "\n",
    "# show some examples\n",
    "print(\"Feature examples:\", feature_names[1700:1710])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the topic model and apply it to the features (words). The topic model uses a mathematical technique called Non-negative Matrix Factorization (NMF) to determine the topics. In this example we distinguish 10 topics, but you can change this number if you want. https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(init='nndsvd', max_iter=2000, n_components=10, random_state=42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a topic model with 10 topics (=n_components)\n",
    "nmf_model = NMF(n_components=10, random_state=42, init='nndsvd', max_iter=2000) \n",
    "\n",
    "# apply/fit the model to the features\n",
    "nmf_model.fit(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will print the first 10 words in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "england robinson rugby wilkinson cup nations captain dawson squad injury\n",
      "\n",
      "Topic #1:\n",
      "economy growth economic rate spending bank dollar said rates consumer\n",
      "\n",
      "Topic #2:\n",
      "seed federer set roddick final open henman moya win beat\n",
      "\n",
      "Topic #3:\n",
      "mr ebbers worldcom sullivan fraud mci accounting trial charges guilty\n",
      "\n",
      "Topic #4:\n",
      "ireland gara penalty try connor driscoll scotland minutes horgan easterby\n",
      "\n",
      "Topic #5:\n",
      "yukos rosneft yugansk oil russian russia court khodorkovsky tax bankruptcy\n",
      "\n",
      "Topic #6:\n",
      "sales euros fiat profits car gm company said firm market\n",
      "\n",
      "Topic #7:\n",
      "wales williams ruddock thomas henson jones italy cardiff france welsh\n",
      "\n",
      "Topic #8:\n",
      "women davenport capriati wimbledon open champion australian prize money equal\n",
      "\n",
      "Topic #9:\n",
      "lions rugby zealand umaga tour woodward new match players hemisphere\n"
     ]
    }
   ],
   "source": [
    "print_topics(nmf_model, nmf_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also check the highest ranked topic for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1: topic: 6\n",
      "\n",
      "doc 2: topic: 1\n",
      "\n",
      "doc 3: topic: 5\n",
      "\n",
      "doc 4: topic: 6\n",
      "\n",
      "doc 5: topic: 6\n",
      "\n",
      "doc 6: topic: 1\n",
      "\n",
      "doc 7: topic: 1\n",
      "\n",
      "doc 8: topic: 1\n",
      "\n",
      "doc 9: topic: 6\n",
      "\n",
      "doc 10: topic: 5\n",
      "\n",
      "doc 11: topic: 6\n",
      "\n",
      "doc 12: topic: 1\n",
      "\n",
      "doc 13: topic: 6\n",
      "\n",
      "doc 14: topic: 6\n",
      "\n",
      "doc 15: topic: 6\n",
      "\n",
      "doc 16: topic: 1\n",
      "\n",
      "doc 17: topic: 6\n",
      "\n",
      "doc 18: topic: 1\n",
      "\n",
      "doc 19: topic: 6\n",
      "\n",
      "doc 20: topic: 6\n",
      "\n",
      "doc 21: topic: 1\n",
      "\n",
      "doc 22: topic: 1\n",
      "\n",
      "doc 23: topic: 1\n",
      "\n",
      "doc 24: topic: 5\n",
      "\n",
      "doc 25: topic: 5\n",
      "\n",
      "doc 26: topic: 1\n",
      "\n",
      "doc 27: topic: 6\n",
      "\n",
      "doc 28: topic: 6\n",
      "\n",
      "doc 29: topic: 6\n",
      "\n",
      "doc 30: topic: 6\n",
      "\n",
      "doc 31: topic: 6\n",
      "\n",
      "doc 32: topic: 6\n",
      "\n",
      "doc 33: topic: 6\n",
      "\n",
      "doc 34: topic: 6\n",
      "\n",
      "doc 35: topic: 6\n",
      "\n",
      "doc 36: topic: 3\n",
      "\n",
      "doc 37: topic: 3\n",
      "\n",
      "doc 38: topic: 1\n",
      "\n",
      "doc 39: topic: 6\n",
      "\n",
      "doc 40: topic: 6\n",
      "\n",
      "doc 41: topic: 6\n",
      "\n",
      "doc 42: topic: 6\n",
      "\n",
      "doc 43: topic: 1\n",
      "\n",
      "doc 44: topic: 1\n",
      "\n",
      "doc 45: topic: 6\n",
      "\n",
      "doc 46: topic: 6\n",
      "\n",
      "doc 47: topic: 1\n",
      "\n",
      "doc 48: topic: 1\n",
      "\n",
      "doc 49: topic: 6\n",
      "\n",
      "doc 50: topic: 1\n",
      "\n",
      "doc 51: topic: 1\n",
      "\n",
      "doc 52: topic: 5\n",
      "\n",
      "doc 53: topic: 6\n",
      "\n",
      "doc 54: topic: 1\n",
      "\n",
      "doc 55: topic: 1\n",
      "\n",
      "doc 56: topic: 6\n",
      "\n",
      "doc 57: topic: 3\n",
      "\n",
      "doc 58: topic: 1\n",
      "\n",
      "doc 59: topic: 3\n",
      "\n",
      "doc 60: topic: 6\n",
      "\n",
      "doc 61: topic: 5\n",
      "\n",
      "doc 62: topic: 6\n",
      "\n",
      "doc 63: topic: 1\n",
      "\n",
      "doc 64: topic: 1\n",
      "\n",
      "doc 65: topic: 1\n",
      "\n",
      "doc 66: topic: 6\n",
      "\n",
      "doc 67: topic: 6\n",
      "\n",
      "doc 68: topic: 6\n",
      "\n",
      "doc 69: topic: 3\n",
      "\n",
      "doc 70: topic: 6\n",
      "\n",
      "doc 71: topic: 6\n",
      "\n",
      "doc 72: topic: 6\n",
      "\n",
      "doc 73: topic: 6\n",
      "\n",
      "doc 74: topic: 6\n",
      "\n",
      "doc 75: topic: 5\n",
      "\n",
      "doc 76: topic: 1\n",
      "\n",
      "doc 77: topic: 6\n",
      "\n",
      "doc 78: topic: 6\n",
      "\n",
      "doc 79: topic: 1\n",
      "\n",
      "doc 80: topic: 6\n",
      "\n",
      "doc 81: topic: 5\n",
      "\n",
      "doc 82: topic: 6\n",
      "\n",
      "doc 83: topic: 6\n",
      "\n",
      "doc 84: topic: 6\n",
      "\n",
      "doc 85: topic: 6\n",
      "\n",
      "doc 86: topic: 1\n",
      "\n",
      "doc 87: topic: 1\n",
      "\n",
      "doc 88: topic: 6\n",
      "\n",
      "doc 89: topic: 1\n",
      "\n",
      "doc 90: topic: 5\n",
      "\n",
      "doc 91: topic: 5\n",
      "\n",
      "doc 92: topic: 1\n",
      "\n",
      "doc 93: topic: 6\n",
      "\n",
      "doc 94: topic: 3\n",
      "\n",
      "doc 95: topic: 3\n",
      "\n",
      "doc 96: topic: 1\n",
      "\n",
      "doc 97: topic: 5\n",
      "\n",
      "doc 98: topic: 1\n",
      "\n",
      "doc 99: topic: 6\n",
      "\n",
      "doc 100: topic: 6\n",
      "\n",
      "doc 101: topic: 1\n",
      "\n",
      "doc 102: topic: 6\n",
      "\n",
      "doc 103: topic: 1\n",
      "\n",
      "doc 104: topic: 6\n",
      "\n",
      "doc 105: topic: 1\n",
      "\n",
      "doc 106: topic: 6\n",
      "\n",
      "doc 107: topic: 1\n",
      "\n",
      "doc 108: topic: 1\n",
      "\n",
      "doc 109: topic: 6\n",
      "\n",
      "doc 110: topic: 1\n",
      "\n",
      "doc 111: topic: 5\n",
      "\n",
      "doc 112: topic: 3\n",
      "\n",
      "doc 113: topic: 1\n",
      "\n",
      "doc 114: topic: 5\n",
      "\n",
      "doc 115: topic: 1\n",
      "\n",
      "doc 116: topic: 1\n",
      "\n",
      "doc 117: topic: 5\n",
      "\n",
      "doc 118: topic: 3\n",
      "\n",
      "doc 119: topic: 6\n",
      "\n",
      "doc 120: topic: 5\n",
      "\n",
      "doc 121: topic: 6\n",
      "\n",
      "doc 122: topic: 1\n",
      "\n",
      "doc 123: topic: 1\n",
      "\n",
      "doc 124: topic: 3\n",
      "\n",
      "doc 125: topic: 5\n",
      "\n",
      "doc 126: topic: 6\n",
      "\n",
      "doc 127: topic: 1\n",
      "\n",
      "doc 128: topic: 1\n",
      "\n",
      "doc 129: topic: 7\n",
      "\n",
      "doc 130: topic: 4\n",
      "\n",
      "doc 131: topic: 7\n",
      "\n",
      "doc 132: topic: 0\n",
      "\n",
      "doc 133: topic: 4\n",
      "\n",
      "doc 134: topic: 0\n",
      "\n",
      "doc 135: topic: 7\n",
      "\n",
      "doc 136: topic: 9\n",
      "\n",
      "doc 137: topic: 9\n",
      "\n",
      "doc 138: topic: 9\n",
      "\n",
      "doc 139: topic: 0\n",
      "\n",
      "doc 140: topic: 9\n",
      "\n",
      "doc 141: topic: 7\n",
      "\n",
      "doc 142: topic: 7\n",
      "\n",
      "doc 143: topic: 0\n",
      "\n",
      "doc 144: topic: 0\n",
      "\n",
      "doc 145: topic: 0\n",
      "\n",
      "doc 146: topic: 0\n",
      "\n",
      "doc 147: topic: 7\n",
      "\n",
      "doc 148: topic: 0\n",
      "\n",
      "doc 149: topic: 7\n",
      "\n",
      "doc 150: topic: 7\n",
      "\n",
      "doc 151: topic: 3\n",
      "\n",
      "doc 152: topic: 7\n",
      "\n",
      "doc 153: topic: 0\n",
      "\n",
      "doc 154: topic: 7\n",
      "\n",
      "doc 155: topic: 0\n",
      "\n",
      "doc 156: topic: 7\n",
      "\n",
      "doc 157: topic: 0\n",
      "\n",
      "doc 158: topic: 0\n",
      "\n",
      "doc 159: topic: 7\n",
      "\n",
      "doc 160: topic: 9\n",
      "\n",
      "doc 161: topic: 4\n",
      "\n",
      "doc 162: topic: 9\n",
      "\n",
      "doc 163: topic: 9\n",
      "\n",
      "doc 164: topic: 0\n",
      "\n",
      "doc 165: topic: 7\n",
      "\n",
      "doc 166: topic: 0\n",
      "\n",
      "doc 167: topic: 0\n",
      "\n",
      "doc 168: topic: 9\n",
      "\n",
      "doc 169: topic: 9\n",
      "\n",
      "doc 170: topic: 0\n",
      "\n",
      "doc 171: topic: 9\n",
      "\n",
      "doc 172: topic: 0\n",
      "\n",
      "doc 173: topic: 4\n",
      "\n",
      "doc 174: topic: 4\n",
      "\n",
      "doc 175: topic: 4\n",
      "\n",
      "doc 176: topic: 0\n",
      "\n",
      "doc 177: topic: 9\n",
      "\n",
      "doc 178: topic: 0\n",
      "\n",
      "doc 179: topic: 4\n",
      "\n",
      "doc 180: topic: 9\n",
      "\n",
      "doc 181: topic: 0\n",
      "\n",
      "doc 182: topic: 9\n",
      "\n",
      "doc 183: topic: 0\n",
      "\n",
      "doc 184: topic: 0\n",
      "\n",
      "doc 185: topic: 0\n",
      "\n",
      "doc 186: topic: 7\n",
      "\n",
      "doc 187: topic: 7\n",
      "\n",
      "doc 188: topic: 9\n",
      "\n",
      "doc 189: topic: 0\n",
      "\n",
      "doc 190: topic: 8\n",
      "\n",
      "doc 191: topic: 0\n",
      "\n",
      "doc 192: topic: 7\n",
      "\n",
      "doc 193: topic: 9\n",
      "\n",
      "doc 194: topic: 7\n",
      "\n",
      "doc 195: topic: 4\n",
      "\n",
      "doc 196: topic: 4\n",
      "\n",
      "doc 197: topic: 0\n",
      "\n",
      "doc 198: topic: 4\n",
      "\n",
      "doc 199: topic: 7\n",
      "\n",
      "doc 200: topic: 9\n",
      "\n",
      "doc 201: topic: 4\n",
      "\n",
      "doc 202: topic: 0\n",
      "\n",
      "doc 203: topic: 7\n",
      "\n",
      "doc 204: topic: 7\n",
      "\n",
      "doc 205: topic: 4\n",
      "\n",
      "doc 206: topic: 0\n",
      "\n",
      "doc 207: topic: 7\n",
      "\n",
      "doc 208: topic: 4\n",
      "\n",
      "doc 209: topic: 4\n",
      "\n",
      "doc 210: topic: 9\n",
      "\n",
      "doc 211: topic: 4\n",
      "\n",
      "doc 212: topic: 2\n",
      "\n",
      "doc 213: topic: 2\n",
      "\n",
      "doc 214: topic: 2\n",
      "\n",
      "doc 215: topic: 2\n",
      "\n",
      "doc 216: topic: 2\n",
      "\n",
      "doc 217: topic: 2\n",
      "\n",
      "doc 218: topic: 2\n",
      "\n",
      "doc 219: topic: 8\n",
      "\n",
      "doc 220: topic: 2\n",
      "\n",
      "doc 221: topic: 8\n",
      "\n",
      "doc 222: topic: 2\n",
      "\n",
      "doc 223: topic: 2\n",
      "\n",
      "doc 224: topic: 2\n",
      "\n",
      "doc 225: topic: 8\n",
      "\n",
      "doc 226: topic: 2\n",
      "\n",
      "doc 227: topic: 8\n",
      "\n",
      "doc 228: topic: 2\n",
      "\n",
      "doc 229: topic: 2\n",
      "\n",
      "doc 230: topic: 2\n",
      "\n",
      "doc 231: topic: 8\n",
      "\n",
      "doc 232: topic: 8\n",
      "\n",
      "doc 233: topic: 8\n",
      "\n",
      "doc 234: topic: 2\n",
      "\n",
      "doc 235: topic: 9\n",
      "\n",
      "doc 236: topic: 8\n",
      "\n",
      "doc 237: topic: 2\n",
      "\n",
      "doc 238: topic: 8\n",
      "\n",
      "doc 239: topic: 2\n",
      "\n",
      "doc 240: topic: 8\n",
      "\n",
      "doc 241: topic: 2\n",
      "\n",
      "doc 242: topic: 2\n",
      "\n",
      "doc 243: topic: 8\n",
      "\n",
      "doc 244: topic: 8\n",
      "\n",
      "doc 245: topic: 2\n",
      "\n",
      "doc 246: topic: 2\n",
      "\n",
      "doc 247: topic: 8\n",
      "\n",
      "doc 248: topic: 2\n",
      "\n",
      "doc 249: topic: 2\n",
      "\n",
      "doc 250: topic: 2\n",
      "\n",
      "doc 251: topic: 2\n",
      "\n",
      "doc 252: topic: 2\n",
      "\n",
      "doc 253: topic: 8\n",
      "\n",
      "doc 254: topic: 2\n",
      "\n",
      "doc 255: topic: 2\n",
      "\n",
      "doc 256: topic: 2\n",
      "\n",
      "doc 257: topic: 2\n",
      "\n",
      "doc 258: topic: 2\n",
      "\n",
      "doc 259: topic: 2\n",
      "\n",
      "doc 260: topic: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_topic = nmf_model.transform(features)\n",
    "\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    print(\"doc {}: topic: {}\\n\".format(n+1,topic_most_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
